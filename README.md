# Heart Attack Risk Prediction â€“ Machine Learning Project

This project builds a complete end-to-end **Heart Attack Risk Prediction** pipeline using machine learning.  
It involves data preprocessing, feature engineering, dimensionality reduction, training classification models,  
and evaluating performance using multiple metrics.

The dataset contains **8,763 samples** and covers demographic, lifestyle, medical measurements, and geographical features.  
The target variable is **Heart Attack Risk (0 or 1)**.

---

## ğŸš€ Project Features

### **1. Data Preprocessing**
- Loaded dataset from Google Drive (Colab environment).
- Cleaned and explored the dataset.
- One-hot encoded categorical variables:
  - `Sex`, `Diet`, `Country`, `Continent`,  
    `Blood Pressure`, `Hemisphere`
- Removed irrelevant identifiers such as:
  - `Patient ID`
- Combined encoded categorical features with numerical ones.
- Final dataset shape after encoding: **3966 features**

---

## ğŸ” Exploratory Data Analysis (EDA)
The project includes:
- Pairplot visualizations (Seaborn)
- Correlation heatmap
- Scatterplots for linearity and correlation

---

## ğŸŒ² Random Forest Feature Importance
To reduce dimensionality:
- Trained `RandomForestClassifier`
- Extracted feature importance for all 3966 features
- Selected all **non-zero importance features**  
  â†’ Reduced feature set significantly while keeping meaningful predictors.

---

## ğŸ“‰ Dimensionality Reduction (LDA)
- Applied **Linear Discriminant Analysis** (LDA)
- Reduced dataset into **one LDA component**  
  (since the dataset is binary classification)
- Visualized:
  - Training LDA distribution
  - Test LDA distribution
  - Combined visualization

---

## ğŸ¤– Model Training
A `RandomForestClassifier` was trained on the LDA-transformed features.

### **Model Performance**
- **Accuracy:** ~0.537  
- **Confusion Matrix**
- **ROC Curve** with AUC
- **Precision-Recall Curve**
- **Threshold vs. F1 Curve**

---

## ğŸ“ˆ Additional Analysis
- Linear regression performed on sample custom data points
- Assessed linearity using:
  - RÂ² value
  - Scatter plot
  - Pearson correlation coefficient

Conclusion: outcome was **non-linear**.

---

## ğŸ“ Project Structure

â”œâ”€â”€ data/
â”‚ â””â”€â”€ heart_attack_prediction_dataset.csv
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ heart_attack_analysis.ipynb
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”œâ”€â”€ feature_engineering.py
â”‚ â”œâ”€â”€ model_training.py
â”‚ â””â”€â”€ evaluation.py
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ heatmap.png
â”‚ â”œâ”€â”€ lda_plot.png
â”‚ â””â”€â”€ roc_curve.png
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

yaml
Copy code

*(You may adjust structure according to your folder layout.)*

---

## ğŸ§ª Technologies Used

| Category | Tools |
|---------|-------|
| Language | Python |
| ML & Preprocessing | scikit-learn, imbalanced-learn |
| Visualization | Matplotlib, Seaborn |
| Environment | Google Colab (+ Google Drive) |
| Data Handling | Pandas, NumPy |

---

## ğŸ“Š Results Summary

| Component | Result |
|----------|---------|
| Model | RandomForestClassifier |
| Dimensionality Reduction | LDA |
| Accuracy | **0.537** |
| ROC-AUC | Plotted |
| Feature Count Before | 3966 |
| Feature Count After RF Reduction | Significantly reduced |
| Data Size | 8,763 rows |

---

## ğŸ”® Future Improvements (Recommended Enhancements)

### âœ” **1. Replace LDA with PCA or UMAP**
LDA works only with linearly separable data.  
Your dataset is **high-dimensional, noisy, and nonlinear**, so PCA or UMAP will give better representation.

### âœ” **2. Feature Engineering for Blood Pressure**
Instead of using thousands of one-hot encoded values like `152/98`:
- Split into Systolic & Diastolic columns  
  â†’ `BP_Systolic`, `BP_Diastolic`

This alone will reduce thousands of columns to **two** useful features.

### âœ” **3. Class Imbalance Handling**
Use:
- SMOTE  
- RandomUnderSampler  
- ADASYN  
- BalancedRandomForest

### âœ” **4. Try Advanced Algorithms**
- XGBoost  
- LightGBM  
- CatBoost (handles categorical features directly)

### âœ” **5. Hyperparameter Optimization**
Use:
- GridSearchCV  
- RandomizedSearchCV  
- Optuna (best option)

### âœ” **6. Model Explainability**
- SHAP (global and local interpretation)
- LIME

### âœ” **7. Deploy the Model**
- Build a Flask/FastAPI API
- Deploy on HuggingFace Spaces or Render
- Create an interactive dashboard using Streamlit

---

## ğŸ“œ License
This project is open-source under the **MIT License**.

---

## ğŸ§‘â€ğŸ’» Author
**Lajim**  
Machine Learning & Data Science Enthusiast  
(Feel free to modify this section)

---

## â­ Contributions
Pull requests are welcome!  
If you use this project, consider giving the repo a **star** â­
